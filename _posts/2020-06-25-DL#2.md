---
layout: post
title: ' [딥러닝] #2 텐서플로우 기초 [2] '
category : [School]
tag: [딥러닝]
---

# Tensorflow 를 사용하는 이유 

다차원 데이터를 다양하게 활용할 수 있음      
딥러닝 라이브러리 인기 순위 1위      

1. Easy model building 
2. Robust production anywhere 
3. Powerful experimentation for research 

## Tensorflow 개념 

* Tensor 를 흘려 보내면서 머신러닝, 딥러닝 알고리즘 수행하는 라이브러리 
* 그래프 구조 : 노드에서 노드로 흘러감 (flow graph) 
  노드 : 수학적 계산, 데이터 읽기, 저장 
  엣지 : 노드 간의 입출력 관계 나타냄 
  
**텐서플로우를 하기 위해서는** 

1. input 값의 상수, 변수, tensor 연산 수행할 node 와 edge 정의 
2. 세션 만들고 node 간의 데이터(tensor) 연산 수행 

![1](https://i.imgur.com/I3oyCUu.png)

노드 : 각각 동그라미 
레이어 : 노드들이 모인 한 층 
네트워크 : 전체적으로 연결된 상태 


## Tensorflow Graph 와 Node 

![2](https://i.imgur.com/uGW9uPB.png)

x = input 
b = bias (편향) 
w = weight (가중치)
sigmoid = activation (활성함수) 
o = output

**인풋값이 들어오면 가중치 곱하고, 편향 더한 다음에, Sigmoid 를 activation으로 사용한 뒤 Output 출력**    

### input 이 하나인 경우 

![3](https://i.imgur.com/VuTmvrO.png)


x = tf.constant([[1.0]]) : x 입력값은 상수로 변하지 않음   
One feature node: 아웃풋을 위해 클래스를 하나 만듦     
tf.Variable : 가중치(w)와 편향(b)은 수시로 변경되기 때문에 변수로 저장해놓음     
tf.math.signoid(out) : 아웃풋을 시그모이드 함수 통해서 출력     


### multi - input 

![4](https://i.imgur.com/uRvX17p.png)

전과 똑같지만  
input 3개 -> result 3개    
가중치(w) 값 2개    


## 순전파 알고리즘 

![5](https://i.imgur.com/NCo9Ygm.png)
입력부터 출력까지 순서대로 진행하여 예측값 출력   

**스스로 예측하면서 탁구(실전연습느낌)**

## 역전파 알고리즘 

![6](https://i.imgur.com/jMEHWV5.png)
예측값과 실제 값을 비교하면서 더이상 오차가 나지 않을 때까지 Back-propagation 하면서 가중치와 Bias 계속 업데이트       

ex) 연구원의 탁구 치기 input 으로 학습 -> 예측과 다른 오류 계산 -> 잘 칠 때까지 back - propagation 하면서 가중치 편향 업데이트      
 
**생각하고 학습함(오답노트 느낌)**

## 순전파와 역전파 graph 

![7](https://i.imgur.com/n98sEQu.png)

preds : **순전파**      
loss : 예측과 실제 값의 차이        
gradient : 다시 업데이트 (loss 를 통해 가중치와 bias 변경) **역전파**       

### MSE (Mean Square Error) 

자주 사용되는 loss fuction     
정답과 아웃풋의 차이를 제곱해서 평균낸 값       

![8](https://i.imgur.com/TZYFmPI.png)      

1 (총 오차) / 4 (총 인풋 갯수) = MSE = 0.25       

## Gradient Descent (Optimization)  

![9](https://i.imgur.com/x5JqK7V.png)    

점점 가중치와 bias 변경하면서 오차를 줄여나가는 방식      


# MNIST 데이터 실습   

## Data processing 

### 모듈 불러오기    

```
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
%matplotlib inline
```

### 데이터 불러오기
tensorflow에서 제공해주는 데이터셋(MNIST)

```
from tensorflow.keras import datasets
```

### 데이터 shape 확인
```
mnist = datasets.mnist
print(mnist)
```
### 데이터 로드 

```
(train_x, train_y), (test_x, test_y) = mnist.load_data()
train_x.shape
>(60000, 28, 28) # 3차원 
```

### 이미지 데이터셋 살펴보기 (train_x)

```
image = train_x[0]
image.shape
> (28, 28)
# 시각화 
plt.imshow(image, 'gray')
plt.show()
```

### 채널 
한번에 많은 데이터를 처리하면 시간이 오래 걸리므로 배치 사이즈 정해야함   
컬러에 따른 채널도 정해야 함   

4차원으로 데이터 차원 수 늘리는 방법   
```
new_train_x = train_x[...,tf.newaxis]
new_train_x.shape
> (60000, 28, 28, 1)
```

하지만 시각화 할때는 차원이 두개여야 함. 그레서 다시 없애야 함  

**첫번째 데이터를 변수에 저장**    
```
disp = new_train_x[0]
disp.shape
> (28, 28, 1) # 0번째 이므로 전체 데이터 갯수 제외 행 열 채널 나옴 
```

**차원 삭제**     
```
disp = np.squeeze(new_train_x[0]) # 맨 마지막 차원 삭제 
disp.shape
>(28,28)
```

**이미지** 
```
plt.imshow(disp, 'gray')
plt.show()
```

### 라벨 데이터 살펴보기 (train_y)

데이터셋 모양 
```
train_y.shape
>(60000,) # 60000개 행으로 이루어짐   
```

라벨과 이미지 함께 시각화 
```
plt.title(train_y[0]) # 0번째 숫자 이름  
plt.imshow(train_x[0], 'gray') # 0번째 숫자 이미지 
plt.show() # 보여주기 
```


### One-hot encoding 

컴퓨터가 이해할 수 있는 형태로 변환해서 라벨 주기 

모듈 임포트 
```
from tensorflow.keras.utils import to_categorical
```

원핫인코딩 
```
label = train_y[0]
label_onehot = to_categorical(label, num_classes=10)
label_onehot
> array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32) # 5를 원핫인코딩으로 표현
```

### 채널 수 늘리기 
   
모델에 들어가기 전에 다시 [batch_size, height, width, channel] 로 구성해야 함     
```
image = train_x[0] #위에서 선언됐던 내용 (첫번째 데이터)
image.shape 
> (28, 28) #원래 모양 
image = image[tf.newaxis, ..., tf.newaxis]
image.shape
> (1, 28, 28, 1) # 차원 추가한 모양 
```

## Feature Extraction 

모든 모델은 1. 특징 추출 (학습) 2. 분류 (예측) 의 두가지 단계로 나눠짐

**구성요소** 

* filter 
* kernel 
* strides 
* padding 
* activation : 활성함수 (ReLU,sigmoid) 

### Convolution 

Feature Extraction 에서 주로 사용    
```
tf.keras.layers.Conv2D(filters= 3, kernel_size=(3,3), strides=(1,1), padding='SAME', activation='relu')
```
몇 개의 필터, 커널 사이즈, 픽셀 한번에 얼만큼 볼건지, 이미지 사이즈 그대로 보장하는 SAME, 활성함수는 바꿔도 상관없음   

더 간단한 방법 
```
tf.keras.layers.Conv2D(3, 3, 1, 'SAME')
```

## 시각화 

image 데이터
```
image = tf.cast(image, dtype=tf.float32) # cast를 이용해서 정수를 실수로 변경해줘야 함   
image.shape
>TensorShape([1, 28, 28, 1])
image
>array([[[[  0], # 5라는 값을 하나로 나열해서 보여줌   
         [  0],
         [  0],
         [  0],
         [  0],
         [  0],
         [  0],
         [  0],
         ...
```

**레이어 적용** 

```
layer = tf.keras.layers.Conv2D(3, 3, 1, padding="SAME")
```

**레이어 적용된 아웃풋** 
이미지에 레이어 적용해서 특징 찾아낸 결과 = output   
```
output = layer(image)
output
> <tf.Tensor: shape=(1, 28, 28, 3), dtype=float32, numpy=
array([[[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...
```

![9](https://i.imgur.com/gvgrztS.png)


원본 이미지와 레이어 적용한 아웃풋 이미지 비교      
```
plt.subplot(1,2,1)
plt.imshow(image[0, :, :, 0], 'gray')
plt.subplot(1,2,2)
plt.imshow(output[0, :, :, 0], 'gray')
plt.show()
```

## Weight 불러오기

0번째 데이터의 가중치 
```
weight = layer.get_weights()
weight[0].shape
> ((3, 3, 1, 3) 
```

가중치 적용 시각화 
```
plt.title(weight[0].shape)
plt.imshow(weight[0] [:, :, 0, 0], 'gray')
```
![10](https://i.imgur.com/6FUtqsL.png)

```
np.min(output), np.max(output)
>(-173.29053, 516.6322)
```

## Activation Function

ReLU 함수 사용 
0보다 작은값은 다 무시하고 높은 값만 사용  
```
tf.keras.layers.ReLU() #ReLU 불러오기 
act_layer = tf.keras.layers.ReLU() # Relu 적용 레이어 생성 
act_output = act_layer(output) # 결과에 대입 
```

그 결과 0보다 작은 값은 무시됨  
```
np.min(act_output), np.max(act_output)
> (0.0, 516.6322)
```

시각화 해보면 아까와는 조금 다른 결과 
```
plt.figure(figsize=(15, 5))
plt.subplot(121)
plt.hist(act_output.numpy(). ravel(), range=[-2,2])
plt.ylim(0, 100)

plt.subplot(122)
plt.title(act_output.shape)
plt.imshow(act_output [0, :, :, 0], 'gray')
plt.colorbar()
plt.show()
```

![11](https://i.imgur.com/Jtbp9hO.png)

불러오기
```
tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding="SAME")
```
레이어에 적용 
```
pool_layer = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding="SAME")
pool_output = pool_layer(act_output)
```
시각화 : 주로 큰 값만 불러옴!   
```
plt.figure(figsize=(15, 5))
plt.subplot(121)
plt.hist(pool_output.numpy(). ravel(), range=[-2,2])
plt.ylim(0, 100)

plt.subplot(122)
plt.title(pool_output.shape)
plt.imshow(pool_output [0, :, :, 0], 'gray')
plt.colorbar()
plt.show()
```

![12](https://i.imgur.com/1v1bhwq.png)
